{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import jax\n",
    "import haiku as hk\n",
    "import jax.numpy as jnp\n",
    "import numpy as onp\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from jax import random\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "from utils.graphics import FOURIER_CMAP\n",
    "from utils.ntk import ntk_eigendecomposition, get_ntk\n",
    "\n",
    "from scipy.io import loadmat, savemat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Models**\n",
    "\n",
    "- Wire-Finer\n",
    "- Gauss-Finer\n",
    "- Finer\n",
    "- Siren\n",
    "- FFN\n",
    "- MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Activation Functions\n",
    "'''\n",
    "scale_fn = lambda x: jax.lax.stop_gradient(jnp.abs(x) + 1)\n",
    "\n",
    "def finer_activation(x, w0=30):\n",
    "    return jnp.sin(w0 * scale_fn(x) * x)\n",
    "\n",
    "def gauss_activation(x, s0=30):\n",
    "    return jnp.exp(-(s0*x)**2)\n",
    "\n",
    "def gaussfiner_activation(x, s0=30, w0_finer=30):\n",
    "    return gauss_activation(finer_activation(x, w0_finer), s0=s0)\n",
    "\n",
    "def wire_activation(x, s0=30, w0=30):\n",
    "    return jnp.exp(1j*w0*x - jnp.abs(s0*x)**2)\n",
    "\n",
    "def wirefiner_activation(x, s0=30, w0=30, w0_finer=30):\n",
    "    return wire_activation(finer_activation(x, w0_finer), s0=s0, w0=w0)\n",
    "\n",
    "''' Plot test\n",
    "'''\n",
    "x = jnp.linspace(-3, 3, 10000) \n",
    "y_gf = gaussfiner_activation(x, s0=2.5, w0_finer=0.1)                    # 1\n",
    "y_wf = wirefiner_activation(x, s0=2.5, w0=5, w0_finer=0.1)         # default:10, 20; 1, 2 \n",
    "plt.figure(figsize=(4*2, 3))\n",
    "plt.subplot(1,2,1); plt.plot(x, y_gf); plt.xlabel('x'); plt.grid(True);\n",
    "plt.subplot(1,2,2); plt.plot(x, y_wf.real); plt.xlabel('x'); plt.grid(True); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' test\\n    s=10; w=20 -> s=2.5; w=5; w_finer=4\\n'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Complex Wire-Finer \n",
    "    Complex Multiply: a+bi * c+di = (ac-bd) + (ad+bc)i\n",
    "'''\n",
    "class WFLayer(hk.Module):\n",
    "    def __init__(self, in_f, out_f, s0=2.5, w0=5, w0_finer=0.4, bs=None, is_first=False, is_last=False):\n",
    "        super().__init__()\n",
    "        self.s0 = s0\n",
    "        self.w0 = w0\n",
    "        self.w0_finer = w0_finer\n",
    "        self.in_f = in_f\n",
    "        self.out_f = out_f\n",
    "        self.is_last=is_last\n",
    "        \n",
    "        # weight init: sine init\n",
    "        self.w_init_bound = 1/in_f if is_first else jnp.sqrt(6/in_f)/(w0*w0_finer)\n",
    "        self.w_dtype = jnp.float32 if is_first else jnp.complex64\n",
    "        \n",
    "        # bias init: pytorch init or bs\n",
    "        self.b_init_bound = 1/jnp.sqrt(in_f) if bs is None else bs\n",
    "        \n",
    "    def __call__(self, x):        \n",
    "        # Complex/Float Weight\n",
    "        w = hk.get_parameter(\"weight\", [self.in_f, self.out_f], dtype=self.w_dtype, \n",
    "                             init=hk.initializers.RandomNormal(-self.w_init_bound, self.w_init_bound))\n",
    "        # Complex/Float Bias\n",
    "        b = hk.get_parameter(\"bias\", [self.out_f], dtype=self.w_dtype, \n",
    "                             init=hk.initializers.RandomNormal(-self.b_init_bound, self.b_init_bound))\n",
    "        # Forward \n",
    "        out = jnp.dot(x, w)\n",
    "        b = jnp.broadcast_to(b, out.shape)\n",
    "        out = out + b\n",
    "        # Activate\n",
    "        return out if self.is_last else wirefiner_activation(out, s0=self.s0, w0=self.w0, w0_finer=self.w0_finer)\n",
    "\n",
    "\n",
    "class WireFINER(hk.Module):\n",
    "    def __init__(self, s0, w0, w0_finer, width, depth, bs=None):\n",
    "        super().__init__()\n",
    "        self.s0 = s0  \n",
    "        self.w0 = w0\n",
    "        self.w0_finer = w0_finer\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.bs = bs\n",
    "\n",
    "    def __call__(self, coords):\n",
    "        sh = coords.shape\n",
    "        x = jnp.reshape(coords, [-1, 1]) # one dimension signal\n",
    "        x = WFLayer(x.shape[-1], self.width, s0=self.s0, w0=self.w0, w0_finer=self.w0_finer, bs=self.bs, is_first=True)(x)\n",
    "        \n",
    "        for _ in range(self.depth - 2):\n",
    "            x = WFLayer(x.shape[-1], self.width, s0=self.s0, w0=self.w0, w0_finer=self.w0_finer)(x)\n",
    "\n",
    "        out = WFLayer(x.shape[-1], 1, s0=self.s0, w0=self.w0, w0_finer=self.w0_finer, is_last=True)(x)\n",
    "        out = jnp.reshape(out, list(sh[:-1]) + [1])\n",
    "        return jnp.real(out) if out.dtype==jnp.complex64 else out\n",
    "\n",
    "''' test\n",
    "    s=10; w=20 -> s=2.5; w=5; w_finer=4\n",
    "'''\n",
    "# model = hk.without_apply_rng(hk.transform(lambda x: WireFINER(s0=2.5, w0=5, w0_finer=4, width=64, depth=3, bs=10)(x)))\n",
    "# coords = jnp.array([1.0, 2.0, 3.0]).reshape(-1, 1); print(coords.shape)\n",
    "# params = model.init(random.PRNGKey(0), jnp.ones((1, 1)))\n",
    "# output = model.apply(params, coords)\n",
    "# print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' test\\n    s=10 -> s=2.5; w_finer=4\\n'"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Gauss-Finer\n",
    "'''\n",
    "class GFLayer(hk.Module):\n",
    "    def __init__(self, in_f, out_f, s0=2.5, w0_finer=2, bs=None, is_first=False, is_last=False):\n",
    "        super().__init__()\n",
    "        self.s0 = s0\n",
    "        self.w0_finer = w0_finer\n",
    "        self.out_f = out_f\n",
    "        self.is_last=is_last\n",
    "        \n",
    "        # weight init: sine init\n",
    "        self.w_init_bound = 1/in_f if is_first else jnp.sqrt(6/in_f)/w0_finer\n",
    "        \n",
    "        # bias init: pytorch init or bs\n",
    "        self.b_init_bound = 1/jnp.sqrt(in_f) if bs is None else bs        \n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = hk.Linear(\n",
    "            output_size=self.out_f,\n",
    "            w_init=hk.initializers.RandomUniform(-self.w_init_bound, self.w_init_bound),\n",
    "            b_init=hk.initializers.RandomUniform(-self.b_init_bound, self.b_init_bound),\n",
    "        )(x)\n",
    "        # Activate\n",
    "        return x if self.is_last else gaussfiner_activation(x, s0=self.s0, w0_finer=self.w0_finer) \n",
    "        \n",
    "        \n",
    "class GaussFINER(hk.Module):\n",
    "    def __init__(self, s0, w0_finer, width, depth, bs=None):\n",
    "        super().__init__()\n",
    "        self.s0 = s0  \n",
    "        self.w0_finer = w0_finer\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.bs = bs\n",
    "\n",
    "    def __call__(self, coords):\n",
    "        sh = coords.shape\n",
    "        x = jnp.reshape(coords, [-1, 1]) # one dimension signal\n",
    "        x = GFLayer(x.shape[-1], self.width, s0=self.s0, w0_finer=self.w0_finer, bs=self.bs, is_first=True)(x)\n",
    "        \n",
    "        for _ in range(self.depth - 2):\n",
    "            x = GFLayer(x.shape[-1], self.width, s0=self.s0, w0_finer=self.w0_finer)(x)\n",
    "\n",
    "        out = GFLayer(x.shape[-1], 1, s0=self.s0, w0_finer=self.w0_finer, is_last=True)(x)\n",
    "        out = jnp.reshape(out, list(sh[:-1]) + [1])\n",
    "        return out\n",
    "\n",
    "''' test\n",
    "    s=10 -> s=2.5; w_finer=4\n",
    "'''\n",
    "# model = hk.without_apply_rng(hk.transform(lambda x: GaussFINER(s0=2.5, w0_finer=4, width=64, depth=3, bs=10)(x)))\n",
    "# coords = jnp.array([1.0, 2.0, 3.0]).reshape(-1, 1); print(coords.shape)\n",
    "# params = model.init(random.PRNGKey(0), jnp.ones((1, 1)))\n",
    "# output = model.apply(params, coords)\n",
    "# print(output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################### FINER ####################################\n",
    "class FinerLayer(hk.Module):\n",
    "    def __init__(self, in_f, out_f, w0=30, bs=None, is_first=False, is_last=False):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "        self.is_first = is_first\n",
    "        self.is_last = is_last\n",
    "        self.out_f = out_f\n",
    "        self.b = 1 / in_f if self.is_first else jnp.sqrt(6 / in_f) / w0\n",
    "        self.b_init = None\n",
    "        if bs:\n",
    "            self.b_init = hk.initializers.RandomUniform(-bs, bs)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = hk.Linear(\n",
    "            output_size=self.out_f,\n",
    "            w_init=hk.initializers.RandomUniform(-self.b, self.b),\n",
    "            b_init=self.b_init,\n",
    "        )(x)\n",
    "        scale = scale_fn(x)\n",
    "        return x + 0.5 if self.is_last else self.w0 * scale * x\n",
    "\n",
    "\n",
    "class FINER(hk.Module):\n",
    "    def __init__(self, w0, width, hidden_w0, depth, bs=None):\n",
    "        super().__init__()\n",
    "        self.w0 = w0  \n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.hidden_w0 = hidden_w0\n",
    "        self.bs = bs\n",
    "\n",
    "    def __call__(self, coords):\n",
    "        sh = coords.shape\n",
    "        x = jnp.reshape(coords, [-1, 1]) # one dimension signal\n",
    "        x = FinerLayer(x.shape[-1], self.width, is_first=True, w0=self.w0, bs=self.bs)(x)\n",
    "        x = jnp.sin(x)\n",
    "\n",
    "        for _ in range(self.depth - 2):\n",
    "            x = FinerLayer(x.shape[-1], self.width, w0=self.hidden_w0)(x)\n",
    "            x = jnp.sin(x)\n",
    "\n",
    "        out = FinerLayer(x.shape[-1], 1, w0=self.hidden_w0, is_last=True)(x)\n",
    "        out = jnp.reshape(out, list(sh[:-1]) + [1])\n",
    "        return out\n",
    "\n",
    "\n",
    "class WIRELayer(hk.Module):\n",
    "    def __init__(self, in_f, out_f, w0=200, s0 = 30, bs=None, is_first=False, is_last=False):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "        self.is_first = is_first\n",
    "        self.in_f = in_f\n",
    "        self.out_f = out_f\n",
    "        self.s0 = s0\n",
    "        self.is_last = is_last\n",
    "\n",
    "    def __call__(self, x):\n",
    "        w = hk.get_parameter(\"weight\", [self.in_f, self.out_f], dtype=jnp.complex64, \n",
    "                             init=hk.initializers.RandomNormal(-jnp.sqrt(1./self.in_f), jnp.sqrt(1./self.in_f)))\n",
    "        # Complex/Float Bias\n",
    "        b = hk.get_parameter(\"bias\", [self.out_f], dtype=jnp.complex64, \n",
    "                             init=hk.initializers.RandomNormal(-jnp.sqrt(1./self.in_f), jnp.sqrt(1./self.in_f)))\n",
    "        out = jnp.dot(x, w)\n",
    "        b = jnp.broadcast_to(b, out.shape)\n",
    "        out = out + b\n",
    "        return out if self.is_last else jnp.exp(1j*self.w0*x - jnp.abs(self.s0*x)**2)\n",
    "\n",
    "class WIRE(hk.Module):\n",
    "    def __init__(self, w0, s0, width, hidden_w0, depth, bs=None):\n",
    "        super().__init__()\n",
    "        self.w0 = w0  # to change the omega_0 of SIREN !!!!\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.hidden_w0 = hidden_w0\n",
    "        self.bs = bs\n",
    "        self.s0 = s0\n",
    "\n",
    "    def __call__(self, coords):\n",
    "        sh = coords.shape\n",
    "        x = jnp.reshape(coords, [-1, 1])\n",
    "        x = WIRELayer(x.shape[-1], self.width, is_first=True, w0=self.w0, s0=self.s0, bs=self.bs)(x)\n",
    "\n",
    "        for _ in range(self.depth - 2):\n",
    "            x = WIRELayer(x.shape[-1], self.width, w0=self.hidden_w0, s0=self.s0)(x)\n",
    "\n",
    "        out = WIRELayer(x.shape[-1], 1, w0=self.hidden_w0, s0=self.s0, is_last=True)(x)\n",
    "        out = jnp.reshape(out, list(sh[:-1]) + [1])\n",
    "\n",
    "        return jnp.real(out)\n",
    "    \n",
    "\n",
    "#################################### SIREN ####################################\n",
    "class SIRENLayer(hk.Module):\n",
    "    def __init__(self, in_f, out_f, w0=200, bs=None, is_first=False, is_last=False):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "        self.is_first = is_first\n",
    "        self.is_last = is_last\n",
    "        self.out_f = out_f\n",
    "        self.b = 1 / in_f if self.is_first else jnp.sqrt(6 / in_f) / w0\n",
    "        self.bi = None\n",
    "        if bs:\n",
    "            self.bi = hk.initializers.RandomUniform(-bs, bs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = hk.Linear(\n",
    "            output_size=self.out_f,\n",
    "            w_init=hk.initializers.RandomUniform(-self.b, self.b),\n",
    "            b_init=self.bi,\n",
    "        )(x)\n",
    "        return x + 0.5 if self.is_last else self.w0 * x\n",
    "    \n",
    "\n",
    "class SIREN(hk.Module):\n",
    "    def __init__(self, w0, width, hidden_w0, depth, bs=None):\n",
    "        super().__init__()\n",
    "        self.w0 = w0  # to change the omega_0 of SIREN !!!!\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.hidden_w0 = hidden_w0\n",
    "        self.bs = bs\n",
    "\n",
    "    def __call__(self, coords):\n",
    "        sh = coords.shape\n",
    "        x = jnp.reshape(coords, [-1, 1])\n",
    "        x = SIRENLayer(x.shape[-1], self.width, is_first=True, w0=self.w0, bs=self.bs)(x)\n",
    "        x = jnp.sin(x)\n",
    "\n",
    "        for _ in range(self.depth - 2):\n",
    "            x = SIRENLayer(x.shape[-1], self.width, w0=self.hidden_w0)(x)\n",
    "            x = jnp.sin(x)\n",
    "\n",
    "        out = SIRENLayer(x.shape[-1], 1, w0=self.hidden_w0, is_last=True)(x)\n",
    "        out = jnp.reshape(out, list(sh[:-1]) + [1])\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "######################################\n",
    "\n",
    "class MLP(hk.Module):\n",
    "    def __init__(self, width, depth):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "\n",
    "    def __call__(self, coords):\n",
    "        sh = coords.shape\n",
    "\n",
    "        x = jnp.reshape(coords, [-1, 1])\n",
    "        x = hk.Linear(self.width)(x)\n",
    "        x = jax.nn.relu(x)\n",
    "\n",
    "        for _ in range(self.depth - 2):\n",
    "            x = hk.Linear(self.width)(x)\n",
    "            x = jax.nn.relu(x)\n",
    "\n",
    "        out = hk.Linear(1)(x)\n",
    "        out = jnp.reshape(out, list(sh[:-1]) + [1])\n",
    "\n",
    "        return out\n",
    "\n",
    "######################################\n",
    "\n",
    "class FFN(hk.Module):\n",
    "    def __init__(self, sigma, width, depth):\n",
    "        super().__init__()\n",
    "        self.sigma = sigma\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.B = self.sigma * jax.random.normal(jax.random.PRNGKey(7), (width, 1))\n",
    "\n",
    "    def __call__(self, coords):\n",
    "        sh = coords.shape\n",
    "        x = jnp.reshape(coords, [-1, 1])\n",
    "        x = input_mapping_fourier(x, self.B)\n",
    "\n",
    "        for _ in range(self.depth - 2):\n",
    "            x = hk.Linear(self.width)(x)\n",
    "            x = jax.nn.relu(x)\n",
    "\n",
    "        out = hk.Linear(1)(x)\n",
    "        out = jnp.reshape(out, list(sh[:-1]) + [1])\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def input_mapping_fourier(x, B):\n",
    "    if B is None:\n",
    "        return x\n",
    "    else:\n",
    "        x_proj = (2.0 * jnp.pi * x) @ B.T\n",
    "        return jnp.concatenate([jnp.sin(x_proj), jnp.cos(x_proj)], axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ntk(\n",
    "    model,\n",
    "    params,\n",
    "    data,\n",
    "    batch_size,\n",
    "    logdir,\n",
    "    keyword=\"\",\n",
    "    colorbar=False,\n",
    "):\n",
    "    outdir = logdir\n",
    "\n",
    "    ntk_matrix = get_ntk(model.apply, params, data, batch_size)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(ntk_matrix, cmap='inferno')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(outdir + os.path.sep + keyword + \"_ntk_matrix\" + \".png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "    if colorbar:\n",
    "        plt.colorbar()\n",
    "    plt.close('all')\n",
    "    \n",
    "    return ntk_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run above'"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''run above'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **NTK Visualization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFAULT_RESOLUTION = 1024\n",
    "# x1 = jnp.linspace(0, 1, DEFAULT_RESOLUTION + 1)[:-1]\n",
    "# # x1 = jnp.linspace(-1, 1, DEFAULT_RESOLUTION + 1)[:-1]\n",
    "# DEFAULT_GRID = x1\n",
    "# BATCH_SIZE = 128\n",
    "# DEFAULT_GRID = jnp.reshape(DEFAULT_GRID, [-1, 1])\n",
    "\n",
    "# width=256\n",
    "# depth=1\n",
    "\n",
    "# logdir = os.path.join(os.getcwd(), 'outputs', 'Finer')\n",
    "# os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "# first_omegas = [1]\n",
    "# hidden_omegas = [30]\n",
    "# bias_scale = [1, 5, 10, 20] \n",
    "\n",
    "# outputs = {}\n",
    "# save_ntk_path = os.path.join(logdir, 'ntks.mat')\n",
    "# if os.path.exists(save_ntk_path):\n",
    "#     outputs = loadmat(save_ntk_path)\n",
    "# for k in outputs:\n",
    "#     print(k)\n",
    "\n",
    "# for hw0 in hidden_omegas: \n",
    "#     for fw0 in first_omegas:\n",
    "#         for bs in bias_scale:\n",
    "#             model_finer = hk.without_apply_rng(hk.transform(lambda x: FINER(w0=fw0, width=width, hidden_w0=hw0, depth=depth, bs=bs)(x)))\n",
    "#             if bs == None:\n",
    "#                 bs = 0\n",
    "            \n",
    "#             key = f'finer_fw{fw0}_hw{hw0}_bs_{bs}_ntkmatrix'\n",
    "#             if key in outputs:\n",
    "#                 continue\n",
    "                \n",
    "#             params_finer = model_finer.init(random.PRNGKey(0), jnp.ones((1, 1)))\n",
    "            \n",
    "#             ntk_matrix = show_ntk(\n",
    "#                 model_finer,\n",
    "#                 params_finer,\n",
    "#                 keyword=f\"finer_bs{bs}\",\n",
    "#                 data=DEFAULT_GRID,\n",
    "#                 batch_size=BATCH_SIZE,\n",
    "#                 logdir=logdir\n",
    "#             )            \n",
    "#             outputs[key] = ntk_matrix\n",
    "            \n",
    "# savemat(save_ntk_path, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Gauss-Finer\n",
    "# '''\n",
    "# DEFAULT_RESOLUTION = 1024\n",
    "# DEFAULT_GRID = jnp.linspace(0, 1, DEFAULT_RESOLUTION + 1)[:-1]\n",
    "# BATCH_SIZE = 128\n",
    "# DEFAULT_GRID = jnp.reshape(DEFAULT_GRID, [-1, 1])\n",
    "\n",
    "# ## params \n",
    "# width=256\n",
    "# depth=1\n",
    "# s0 = 2.5\n",
    "# w0_finer = 0.4\n",
    "# bias_scale = [1, 5, 10, 20] \n",
    "\n",
    "# expname = f'gaussfiner_s0[{s0}]_w0f[{w0_finer}]'\n",
    "\n",
    "# ## logdir \n",
    "# logdir = os.path.join(os.getcwd(), 'outputs', expname); os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "# ## save \n",
    "# outputs = {}\n",
    "# save_ntk_path = os.path.join(logdir, f'{expname}_ntks.mat')\n",
    "# if os.path.exists(save_ntk_path):\n",
    "#     outputs = loadmat(save_ntk_path)\n",
    "# # for k in outputs:\n",
    "# #     print(k)\n",
    "    \n",
    "# ## Run\n",
    "# for bs in bias_scale:\n",
    "#     model = hk.without_apply_rng(hk.transform(lambda x: GaussFINER(s0=s0, w0_finer=w0_finer, width=width, depth=depth, bs=bs)(x)))\n",
    "    \n",
    "#     key = f'net_bs_{bs}'\n",
    "#     if key in outputs:\n",
    "#         continue\n",
    "        \n",
    "#     params = model.init(random.PRNGKey(0), jnp.ones((1, 1)))\n",
    "#     ntk_matrix = show_ntk(\n",
    "#         model,\n",
    "#         params,\n",
    "#         keyword=key,\n",
    "#         data=DEFAULT_GRID,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         logdir=logdir \n",
    "#     )\n",
    "#     print(ntk_matrix.max(), ntk_matrix.min())\n",
    "#     outputs[key] = ntk_matrix\n",
    "\n",
    "# savemat(save_ntk_path, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Wire-Finer\n",
    "# '''\n",
    "# DEFAULT_RESOLUTION = 1024\n",
    "# DEFAULT_GRID = jnp.linspace(0, 1, DEFAULT_RESOLUTION + 1)[:-1]\n",
    "# BATCH_SIZE = 128\n",
    "# DEFAULT_GRID = jnp.reshape(DEFAULT_GRID, [-1, 1])\n",
    "\n",
    "# ## params \n",
    "# width=256\n",
    "# depth=1\n",
    "# s0 = 2.5\n",
    "# w0 = 5\n",
    "# w0_finer = 0.08\n",
    "# bias_scale = [1, 5, 10, 20] \n",
    "\n",
    "# expname = f'wirefiner_s0[{s0}]_w0[{w0}]_w0f[{w0_finer}]'\n",
    "\n",
    "# ## logdir \n",
    "# logdir = os.path.join(os.getcwd(), 'outputs', expname); os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "# ## save \n",
    "# outputs = {}\n",
    "# save_ntk_path = os.path.join(logdir, f'{expname}_ntk.mat')\n",
    "# if os.path.exists(save_ntk_path):\n",
    "#     outputs = loadmat(save_ntk_path)\n",
    "# # for k in outputs:\n",
    "# #     print(k)\n",
    "    \n",
    "# ## Run\n",
    "# for bs in bias_scale:\n",
    "#     model = hk.without_apply_rng(hk.transform(lambda x: WireFINER(s0=s0, w0=w0, w0_finer=w0_finer, width=width, depth=depth, bs=bs)(x)))\n",
    "    \n",
    "#     key = f'net_bs_{bs}'\n",
    "#     if key in outputs:\n",
    "#         continue\n",
    "        \n",
    "#     params = model.init(random.PRNGKey(0), jnp.ones((1, 1)))\n",
    "#     ntk_matrix = show_ntk(\n",
    "#         model,\n",
    "#         params,\n",
    "#         keyword=key,\n",
    "#         data=DEFAULT_GRID,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#         logdir=logdir \n",
    "#     )\n",
    "#     print(ntk_matrix.max(), ntk_matrix.min())\n",
    "#     outputs[key] = ntk_matrix\n",
    "\n",
    "# savemat(save_ntk_path, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## SIREN\n",
    "# fw0 = 1\n",
    "# hw0 = 30\n",
    "# bias_scale = [None, 0.5, 1, 3, 5, 10, 20, 30, 40]\n",
    "\n",
    "# for bs in bias_scale:\n",
    "#     model_SIREN = hk.without_apply_rng(hk.transform(lambda x: SIREN(w0=fw0, width=width, hidden_w0=hw0, depth=depth, bs=bs)(x)))\n",
    "#     params_SIREN = model_SIREN.init(random.PRNGKey(0), jnp.ones((1, 1)))\n",
    "    \n",
    "#     if bs == None:\n",
    "#         bs = 0\n",
    "        \n",
    "#     key = f'siren_fw{fw0}_hw{hw0}_bs_{bs}_ntkmatrix'\n",
    "#     if key in outputs:\n",
    "#         continue        \n",
    "    \n",
    "#     ntk_matrix = show_ntk(\n",
    "#         model_SIREN,\n",
    "#         params_SIREN,\n",
    "#         keyword=f\"siren_bs_{bs}\",\n",
    "#         data=DEFAULT_GRID,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#     )\n",
    "    \n",
    "#     outputs[key] = ntk_matrix\n",
    "#     print(ntk_matrix.max(), ntk_matrix.min())\n",
    "\n",
    "# savemat(save_ntk_path, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0 0.63167024\n"
     ]
    }
   ],
   "source": [
    "## Wire\n",
    "w0 = 20\n",
    "s0 = 10\n",
    "depth = 1\n",
    "width = 1024\n",
    "DEFAULT_RESOLUTION = 1024\n",
    "DEFAULT_GRID = jnp.linspace(-1, 1, DEFAULT_RESOLUTION + 1)[:-1]\n",
    "BATCH_SIZE =1024\n",
    "DEFAULT_GRID = jnp.reshape(DEFAULT_GRID, [-1, 1])\n",
    "logdir = os.path.join('outputs', 'Wire')\n",
    "os.makedirs(logdir, exist_ok=True)\n",
    "\n",
    "model_Wire = hk.without_apply_rng(hk.transform(lambda x: WIRE(w0=w0, s0=s0, hidden_w0=w0, width=width, depth=depth)(x)))\n",
    "params_Wire = model_Wire.init(random.PRNGKey(0), jnp.ones((1, 1)))\n",
    "    \n",
    "key = f'wire_w0{w0}_s0{s0}_ntkmatrix'\n",
    "\n",
    "ntk_matrix = show_ntk(\n",
    "    model_Wire,\n",
    "    params_Wire,\n",
    "    keyword=f\"wire_w0{w0}_s0{s0}\",\n",
    "    data=DEFAULT_GRID,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    logdir=logdir\n",
    ")\n",
    "\n",
    "print(ntk_matrix.max(), ntk_matrix.min())\n",
    "\n",
    "# savemat(save_ntk_path, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## MLP\n",
    "# model_mlp = hk.without_apply_rng(hk.transform(lambda x: MLP(width=width, depth=depth)(x)))\n",
    "# params_mlp = model_mlp.init(random.PRNGKey(0), jnp.ones((1, 1)))\n",
    "\n",
    "# key = f'mlp_ntkmatrix'\n",
    "# if key not in outputs:\n",
    "#     ntk_matrix = show_ntk(\n",
    "#         model_mlp,\n",
    "#         params_mlp,\n",
    "#         keyword=f\"mlp\",\n",
    "#         data=DEFAULT_GRID,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#     )\n",
    "#     outputs[key] = ntk_matrix\n",
    "#     print(ntk_matrix.max(), ntk_matrix.min())\n",
    "#     savemat(save_ntk_path, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## FFN: OK \n",
    "# model_FFN_1 = hk.without_apply_rng(hk.transform(lambda x: FFN(sigma=1, width=width, depth=depth)(x)))\n",
    "# params_FFN_1 = model_FFN_1.init(random.PRNGKey(0), jnp.ones((1, 1)))\n",
    "\n",
    "# model_FFN_10 = hk.without_apply_rng(hk.transform(lambda x: FFN(sigma=10, width=width, depth=depth)(x)))\n",
    "# params_FFN_10 = model_FFN_10.init(random.PRNGKey(0), jnp.ones((1, 1)))\n",
    "\n",
    "# key = f'ffn_10_ntkmatrix'\n",
    "# if key not in outputs:\n",
    "#     ntk_matrix_ffn_10 = show_ntk(\n",
    "#         model_FFN_10,\n",
    "#         params_FFN_10,\n",
    "#         keyword=\"ffn_10\",\n",
    "#         data=DEFAULT_GRID,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#     )\n",
    "#     outputs[key] = ntk_matrix_ffn_10\n",
    "#     savemat(save_ntk_path, outputs)\n",
    "\n",
    "# key = f'ffn_1_ntkmatrix'\n",
    "# if key not in outputs:\n",
    "#     ntk_matrix_ffn_1 = show_ntk(\n",
    "#         model_FFN_1,\n",
    "#         params_FFN_1,\n",
    "#         keyword=\"ffn_1\",\n",
    "#         data=DEFAULT_GRID,\n",
    "#         batch_size=BATCH_SIZE,\n",
    "#     )\n",
    "#     outputs[key] = ntk_matrix_ffn_1\n",
    "#     savemat(save_ntk_path, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## test\n",
    "# divmax_logdir = os.path.join(logdir, 'DivMax')\n",
    "# os.makedirs(divmax_logdir, exist_ok=True)\n",
    "\n",
    "# outputs = loadmat(save_ntk_path)\n",
    "# for k in outputs:\n",
    "#     if k in ['__header__', '__version__', '__globals__']:\n",
    "#         continue\n",
    "#     ntk_mat = outputs[k]\n",
    "#     print(k, '    ', ntk_mat.max(), ntk_mat.min())\n",
    "#     # ntk_mat /= 260\n",
    "    \n",
    "#     norm = matplotlib.colors.Normalize(vmin=0, vmax=507)\n",
    "    \n",
    "#     plt.imshow(ntk_mat, cmap='inferno', norm=norm)\n",
    "#     plt.colorbar()\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(divmax_logdir + os.path.sep + k + \"_ntk_matrix.png\", bbox_inches=\"tight\", pad_inches=0)\n",
    "#     plt.close('all')\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as onp\n",
    "# from scipy.io import savemat, loadmat\n",
    "\n",
    "# # finer\n",
    "# outs = onp.load('ntk_mat/ntks.npy', allow_pickle=True).item()\n",
    "# for k in outs:\n",
    "#     print(k)\n",
    "\n",
    "# savemat('ntk_mat/ntks_finer.mat', outs)\n",
    "\n",
    "# # siren\n",
    "# outs = onp.load('ntk_mat/ntks_siren.npy', allow_pickle=True).item()\n",
    "# for k in outs:\n",
    "#     print(k)\n",
    "# savemat('ntk_mat/ntks_siren.mat', outs)\n",
    "\n",
    "\n",
    "# mats = loadmat('ntk_mat/ntks_finer.mat')\n",
    "\n",
    "# # for k in mats:\n",
    "# #     print(k)\n",
    "    \n",
    "    \n",
    "# mats = loadmat('ntk_mat/ntks_siren.mat')\n",
    "\n",
    "# for k in mats:\n",
    "#     print(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inr_dictionaries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
